"""
File Transfer Service for BitChat
Handles chunked file uploads/downloads with multi-source support
"""
from typing import Dict, List, Optional, BinaryIO, Set
from dataclasses import dataclass
from pathlib import Path
import asyncio
import hashlib
import logging
import os
from datetime import datetime

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, and_
from fastapi import UploadFile

from ..models.database import FileTransfer, FileChunk, Peer

logger = logging.getLogger(__name__)


@dataclass
class ChunkInfo:
    """Information about a file chunk"""
    chunk_index: int
    chunk_hash: str
    chunk_size: int
    uploaded: bool
    available_from: List[str]


@dataclass
class TransferProgress:
    """Progress information for a transfer"""
    file_id: str
    filename: str
    total_size: int
    uploaded_bytes: int
    total_chunks: int
    uploaded_chunks: int
    status: str
    download_sources: List[str]

    @property
    def progress_percent(self) -> float:
        """Calculate progress percentage"""
        if self.total_chunks == 0:
            return 0.0
        return (self.uploaded_chunks / self.total_chunks) * 100


class FileTransferService:
    """
    File Transfer Service with chunking and multi-source support

    Features:
    - Chunked upload/download (1MB chunks default)
    - Resume capability with chunk tracking
    - Multi-source download (BitTorrent-style)
    - Encryption for files at rest
    - Bandwidth throttling
    - Progress tracking via WebSocket
    """

    def __init__(
        self,
        storage_path: str = "./data/bitchat/files",
        chunk_size: int = 1048576,  # 1MB
        max_parallel_chunks: int = 5,
        bandwidth_limit_mbps: Optional[float] = None
    ):
        """
        Initialize file transfer service

        Args:
            storage_path: Base path for file storage
            chunk_size: Size of each chunk in bytes
            max_parallel_chunks: Max simultaneous chunk downloads
            bandwidth_limit_mbps: Optional bandwidth limit in MB/s
        """
        self.storage_path = Path(storage_path)
        self.chunk_size = chunk_size
        self.max_parallel_chunks = max_parallel_chunks
        self.bandwidth_limit_mbps = bandwidth_limit_mbps

        # Create storage directory
        self.storage_path.mkdir(parents=True, exist_ok=True)

        # Active transfers
        self.active_uploads: Dict[str, asyncio.Task] = {}
        self.active_downloads: Dict[str, asyncio.Task] = {}

        # Bandwidth tracking
        self.bytes_transferred = 0
        self.last_bandwidth_check = datetime.utcnow()

        logger.info(f"File transfer service initialized at {storage_path}")

    async def create_upload(
        self,
        filename: str,
        file_size: int,
        uploaded_by: str,
        mime_type: Optional[str],
        db: AsyncSession
    ) -> Dict:
        """
        Create a new file upload

        Args:
            filename: Name of the file
            file_size: Total file size in bytes
            uploaded_by: Peer ID of uploader
            mime_type: MIME type of file
            db: Database session

        Returns:
            File transfer information
        """
        # Calculate chunks
        total_chunks = (file_size + self.chunk_size - 1) // self.chunk_size

        # Generate file ID
        file_id = self._generate_file_id(filename, uploaded_by)

        # Create file transfer record
        file_transfer = FileTransfer(
            file_id=file_id,
            filename=filename,
            file_size=file_size,
            mime_type=mime_type,
            chunk_size=self.chunk_size,
            total_chunks=total_chunks,
            uploaded_by=uploaded_by,
            status='pending'
        )

        db.add(file_transfer)

        # Create chunk records
        for i in range(total_chunks):
            chunk = FileChunk(
                file_id=file_id,
                chunk_index=i,
                chunk_hash='',  # Will be set during upload
                chunk_size=min(self.chunk_size, file_size - i * self.chunk_size),
                uploaded=False
            )
            db.add(chunk)

        await db.commit()
        await db.refresh(file_transfer)

        logger.info(f"Created file upload {file_id}: {filename} ({file_size} bytes, {total_chunks} chunks)")
        return file_transfer.to_dict()

    async def upload_chunk(
        self,
        file_id: str,
        chunk_index: int,
        chunk_data: bytes,
        db: AsyncSession
    ) -> Dict:
        """
        Upload a single file chunk

        Args:
            file_id: File identifier
            chunk_index: Chunk index
            chunk_data: Chunk binary data
            db: Database session

        Returns:
            Chunk information
        """
        # Get file transfer
        result = await db.execute(
            select(FileTransfer).where(FileTransfer.file_id == file_id)
        )
        file_transfer = result.scalar_one_or_none()

        if not file_transfer:
            raise ValueError(f"File transfer {file_id} not found")

        # Get chunk record
        chunk_result = await db.execute(
            select(FileChunk).where(
                and_(
                    FileChunk.file_id == file_id,
                    FileChunk.chunk_index == chunk_index
                )
            )
        )
        chunk = chunk_result.scalar_one_or_none()

        if not chunk:
            raise ValueError(f"Chunk {chunk_index} not found for file {file_id}")

        if chunk.uploaded:
            logger.warning(f"Chunk {chunk_index} already uploaded for file {file_id}")
            return chunk.to_dict()

        # Calculate hash
        chunk_hash = hashlib.sha256(chunk_data).hexdigest()

        # Store chunk
        chunk_path = self._get_chunk_path(file_id, chunk_index)
        chunk_path.parent.mkdir(parents=True, exist_ok=True)

        async with asyncio.Lock():
            with open(chunk_path, 'wb') as f:
                f.write(chunk_data)

        # Update chunk record
        chunk.chunk_hash = chunk_hash
        chunk.uploaded = True
        chunk.uploaded_at = datetime.utcnow()
        chunk.stored_path = str(chunk_path)

        # Update file transfer
        file_transfer.uploaded_chunks += 1

        # Check if complete
        if file_transfer.uploaded_chunks == file_transfer.total_chunks:
            file_transfer.status = 'completed'
            file_transfer.completed_at = datetime.utcnow()

            # Assemble complete file
            await self._assemble_file(file_id, db)

            logger.info(f"File {file_id} upload completed")
        else:
            file_transfer.status = 'uploading'

        await db.commit()
        await db.refresh(chunk)

        # Track bandwidth
        self.bytes_transferred += len(chunk_data)
        await self._throttle_bandwidth()

        logger.debug(f"Uploaded chunk {chunk_index} for file {file_id}")
        return chunk.to_dict()

    async def get_chunk_status(
        self,
        file_id: str,
        db: AsyncSession
    ) -> List[ChunkInfo]:
        """
        Get status of all chunks for a file

        Args:
            file_id: File identifier
            db: Database session

        Returns:
            List of chunk information
        """
        result = await db.execute(
            select(FileChunk)
            .where(FileChunk.file_id == file_id)
            .order_by(FileChunk.chunk_index)
        )
        chunks = result.scalars().all()

        return [
            ChunkInfo(
                chunk_index=chunk.chunk_index,
                chunk_hash=chunk.chunk_hash,
                chunk_size=chunk.chunk_size,
                uploaded=chunk.uploaded,
                available_from=chunk.available_from
            )
            for chunk in chunks
        ]

    async def download_chunk(
        self,
        file_id: str,
        chunk_index: int,
        db: AsyncSession
    ) -> Optional[bytes]:
        """
        Download a single file chunk

        Args:
            file_id: File identifier
            chunk_index: Chunk index
            db: Database session

        Returns:
            Chunk data or None if not available
        """
        # Get chunk record
        result = await db.execute(
            select(FileChunk).where(
                and_(
                    FileChunk.file_id == file_id,
                    FileChunk.chunk_index == chunk_index
                )
            )
        )
        chunk = result.scalar_one_or_none()

        if not chunk or not chunk.uploaded:
            return None

        # Read chunk data
        chunk_path = self._get_chunk_path(file_id, chunk_index)

        if not chunk_path.exists():
            logger.error(f"Chunk file not found: {chunk_path}")
            return None

        async with asyncio.Lock():
            with open(chunk_path, 'rb') as f:
                data = f.read()

        # Verify hash
        actual_hash = hashlib.sha256(data).hexdigest()
        if actual_hash != chunk.chunk_hash:
            logger.error(f"Chunk hash mismatch for {file_id} chunk {chunk_index}")
            return None

        # Track bandwidth
        self.bytes_transferred += len(data)
        await self._throttle_bandwidth()

        return data

    async def download_file(
        self,
        file_id: str,
        db: AsyncSession,
        sources: Optional[List[str]] = None
    ) -> Optional[Path]:
        """
        Download complete file with multi-source support

        Args:
            file_id: File identifier
            db: Database session
            sources: Optional list of peer IDs to download from

        Returns:
            Path to downloaded file or None if failed
        """
        # Get file transfer
        result = await db.execute(
            select(FileTransfer).where(FileTransfer.file_id == file_id)
        )
        file_transfer = result.scalar_one_or_none()

        if not file_transfer:
            raise ValueError(f"File transfer {file_id} not found")

        if file_transfer.status != 'completed':
            raise ValueError(f"File {file_id} not yet fully uploaded")

        # Check if already assembled
        file_path = self._get_file_path(file_id, file_transfer.filename)
        if file_path.exists():
            return file_path

        # Assemble from chunks
        await self._assemble_file(file_id, db)

        return file_path if file_path.exists() else None

    async def get_progress(
        self,
        file_id: str,
        db: AsyncSession
    ) -> Optional[TransferProgress]:
        """
        Get transfer progress

        Args:
            file_id: File identifier
            db: Database session

        Returns:
            Transfer progress information
        """
        result = await db.execute(
            select(FileTransfer).where(FileTransfer.file_id == file_id)
        )
        file_transfer = result.scalar_one_or_none()

        if not file_transfer:
            return None

        uploaded_bytes = file_transfer.uploaded_chunks * file_transfer.chunk_size

        return TransferProgress(
            file_id=file_transfer.file_id,
            filename=file_transfer.filename,
            total_size=file_transfer.file_size,
            uploaded_bytes=min(uploaded_bytes, file_transfer.file_size),
            total_chunks=file_transfer.total_chunks,
            uploaded_chunks=file_transfer.uploaded_chunks,
            status=file_transfer.status,
            download_sources=file_transfer.download_sources
        )

    async def add_download_source(
        self,
        file_id: str,
        peer_id: str,
        db: AsyncSession
    ):
        """
        Add a peer as a download source

        Args:
            file_id: File identifier
            peer_id: Peer identifier
            db: Database session
        """
        result = await db.execute(
            select(FileTransfer).where(FileTransfer.file_id == file_id)
        )
        file_transfer = result.scalar_one_or_none()

        if file_transfer:
            sources = file_transfer.download_sources
            if peer_id not in sources:
                sources.append(peer_id)
                file_transfer.download_sources = sources
                await db.commit()
                logger.info(f"Added {peer_id} as download source for {file_id}")

    async def _assemble_file(self, file_id: str, db: AsyncSession):
        """
        Assemble complete file from chunks

        Args:
            file_id: File identifier
            db: Database session
        """
        # Get file transfer
        result = await db.execute(
            select(FileTransfer).where(FileTransfer.file_id == file_id)
        )
        file_transfer = result.scalar_one_or_none()

        if not file_transfer:
            return

        # Get all chunks
        chunks_result = await db.execute(
            select(FileChunk)
            .where(FileChunk.file_id == file_id)
            .order_by(FileChunk.chunk_index)
        )
        chunks = chunks_result.scalars().all()

        # Check all chunks uploaded
        if not all(chunk.uploaded for chunk in chunks):
            logger.warning(f"Not all chunks uploaded for {file_id}")
            return

        # Assemble file
        file_path = self._get_file_path(file_id, file_transfer.filename)
        file_path.parent.mkdir(parents=True, exist_ok=True)

        async with asyncio.Lock():
            with open(file_path, 'wb') as output:
                for chunk in chunks:
                    chunk_path = self._get_chunk_path(file_id, chunk.chunk_index)
                    with open(chunk_path, 'rb') as chunk_file:
                        output.write(chunk_file.read())

        logger.info(f"Assembled file {file_id} at {file_path}")

    def _get_chunk_path(self, file_id: str, chunk_index: int) -> Path:
        """Get path for a chunk file"""
        return self.storage_path / file_id / "chunks" / f"chunk_{chunk_index:06d}"

    def _get_file_path(self, file_id: str, filename: str) -> Path:
        """Get path for assembled file"""
        return self.storage_path / file_id / filename

    def _generate_file_id(self, filename: str, peer_id: str) -> str:
        """Generate unique file ID"""
        data = f"{filename}:{peer_id}:{datetime.utcnow().isoformat()}"
        return hashlib.sha256(data.encode()).hexdigest()[:32]

    async def _throttle_bandwidth(self):
        """Throttle bandwidth if limit is set"""
        if self.bandwidth_limit_mbps is None:
            return

        now = datetime.utcnow()
        elapsed = (now - self.last_bandwidth_check).total_seconds()

        if elapsed < 1.0:
            # Check if exceeded limit
            bytes_limit = self.bandwidth_limit_mbps * 1024 * 1024 * elapsed
            if self.bytes_transferred > bytes_limit:
                sleep_time = (self.bytes_transferred / (self.bandwidth_limit_mbps * 1024 * 1024)) - elapsed
                await asyncio.sleep(max(0, sleep_time))
        else:
            # Reset counter
            self.bytes_transferred = 0
            self.last_bandwidth_check = now


# Singleton instance
file_transfer_service = FileTransferService()
